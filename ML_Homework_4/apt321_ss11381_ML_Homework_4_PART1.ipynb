{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1512cb60f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# creating the function and plotting it \n",
    "function = lambda x: (2*(x**4))-(2*(x**3))-(12*(x**2))+6\n",
    "\n",
    "# Get 1000 evenly spaced numbers between -4 and 4 (arbitratil chosen to ensure steep curve)\n",
    "x = np.linspace(-4,4,1000)\n",
    "\n",
    "# Plot the curve\n",
    "plt.plot(x, function(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. What is the value of x at the local minimum and at the global minimum? Find the answer to this question like either by hand with calculus or using matlab or other software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -4.348958\n",
      "         Iterations: 18\n",
      "         Function evaluations: 37\n",
      "Local Minima at x =  -1.3971679687499976\n",
      "Global Minima at x =  2.1471808637959735\n"
     ]
    }
   ],
   "source": [
    "# Verified at: https://www.symbolab.com/solver/calculus-function-extreme-points-calculator/extreme%20f%5Cleft(x%5Cright)%3D%202x%5E%7B4%7D%20%E2%88%92%202x%5E%7B3%7D%20%E2%88%92%2012x%5E%7B2%7D%20%2B%206\n",
    "# Local Minima\n",
    "from scipy.optimize import fmin\n",
    "print(\"Local Minima at x = \",fmin(function,-4)[0])\n",
    "\n",
    "# Global Minima\n",
    "from scipy import optimize\n",
    "x_min = optimize.brent(function)\n",
    "print(\"Global Minima at x = \",optimize.brent(function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Perform Gradient Descent,  Setting x = −4 and η = 0.001, run gradient descent for 6 iterations (that is, do the update 6 times). Report the values of x and f(x) at the start and after each of the first 6 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To find derivative: http://www.learningaboutelectronics.com/Articles/How-to-find-the-derivative-of-a-function-in-Python.php\n",
    "# The algorithm starts at x=-4\n",
    "cur_x = -4 \n",
    "\n",
    "# Learning rate\n",
    "rate = 0.001 \n",
    "\n",
    "#This tells us when to stop the algorithm\n",
    "precision = 0.000001 \n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iters = 6 \n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# Gradient of our function \n",
    "df = lambda x: (8*(x**3)) - (6*(x**2)) - (24*x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before entering the iteration x is: -2.4794003442716166, f(x) is: 38.29644231132754\n",
      "\n",
      "\n",
      "The local minimum occurs at -2.4794003442716166\n"
     ]
    }
   ],
   "source": [
    "print(\"Before entering the iteration x is: \"+str(cur_x)+\", f(x) is: \"+ str(function(cur_x))+\"\\n\")\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    # Store current x value in prev_x\n",
    "    prev_x = cur_x\n",
    "    # Grad descent\n",
    "    cur_x = cur_x - rate * df(prev_x)\n",
    "    # Change in x\n",
    "    previous_step_size = abs(cur_x - prev_x) \n",
    "    # iteration count\n",
    "    iters = iters+1\n",
    "    # Print iterations\n",
    "    print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x))) \n",
    "\n",
    "print(\"\\nThe local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1200 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find derivative: http://www.learningaboutelectronics.com/Articles/How-to-find-the-derivative-of-a-function-in-Python.php\n",
    "# The algorithm starts at x=-4\n",
    "cur_x = -4 \n",
    "\n",
    "# Learning rate\n",
    "rate = 0.001 \n",
    "\n",
    "#This tells us when to stop the algorithm\n",
    "precision = 0.000001 \n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iters = 1200\n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# Gradient of our function \n",
    "df = lambda x: (8*(x**3)) - (6*(x**2)) - (24*x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before entering the iteration x is: -4, f(x) is: 454\n",
      "\n",
      "Iteration 246: X = -1.3972104037610875, f(x) = -4.348957706810287\n",
      "Iteration 247: X = -1.3972092332877633, f(x) = -4.348957708153158\n",
      "Iteration 248: X = -1.397208109187661, f(x) = -4.348957709391726\n",
      "Iteration 249: X = -1.3972070296234085, f(x) = -4.3489577105340995\n",
      "Iteration 250: X = -1.397205992830441, f(x) = -4.348957711587744\n",
      "Iteration 251: X = -1.397204997114115, f(x) = -4.348957712559557\n",
      "\n",
      "The local minimum occurs at -1.397204997114115\n"
     ]
    }
   ],
   "source": [
    "print(\"Before entering the iteration x is: \"+str(cur_x)+\", f(x) is: \"+ str(function(cur_x))+\"\\n\")\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    # Store current x value in prev_x\n",
    "    prev_x = cur_x\n",
    "    # Grad descent\n",
    "    cur_x = cur_x - rate * df(prev_x)\n",
    "    # Change in x\n",
    "    previous_step_size = abs(cur_x - prev_x) \n",
    "    # iteration count\n",
    "    iters = iters+1\n",
    "    # Iteration stopped at 251, showing last 6 Iterations\n",
    "    if iters>=246:\n",
    "        # Print iterations\n",
    "        print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x))) \n",
    "    \n",
    "print(\"\\nThe local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Repeat the previous exercise, but this time, start with x=4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To find derivative: http://www.learningaboutelectronics.com/Articles/How-to-find-the-derivative-of-a-function-in-Python.php\n",
    "# The algorithm starts at x=-4\n",
    "cur_x = 4 \n",
    "\n",
    "# Learning rate\n",
    "rate = 0.001 \n",
    "\n",
    "#This tells us when to stop the algorithm\n",
    "precision = 0.000001 \n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iters = 6\n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# Gradient of our function \n",
    "df = lambda x: (8*(x**3)) - (6*(x**2)) - (24*x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before entering the iteration x is: 2.9312689375235244, f(x) is: 0.17557478693807127\n",
      "\n",
      "\n",
      "The local minimum occurs at 2.9312689375235244\n"
     ]
    }
   ],
   "source": [
    "print(\"Before entering the iteration x is: \"+str(cur_x)+\", f(x) is: \"+ str(function(cur_x))+\"\\n\")\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    # Store current x value in prev_x\n",
    "    prev_x = cur_x\n",
    "    # Grad descent\n",
    "    cur_x = cur_x - rate * df(prev_x)\n",
    "    # Change in x\n",
    "    previous_step_size = abs(cur_x - prev_x) \n",
    "    # iteration count\n",
    "    iters = iters+1\n",
    "    # Print iterations\n",
    "    print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x))) \n",
    "\n",
    "print(\"\\nThe local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1200 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To find derivative: http://www.learningaboutelectronics.com/Articles/How-to-find-the-derivative-of-a-function-in-Python.php\n",
    "# The algorithm starts at x=-4\n",
    "cur_x = 4 \n",
    "\n",
    "# Learning rate\n",
    "rate = 0.001 \n",
    "\n",
    "#This tells us when to stop the algorithm\n",
    "precision = 0.000001 \n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iters = 1200\n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# Gradient of our function \n",
    "df = lambda x: (8*(x**3)) - (6*(x**2)) - (24*x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before entering the iteration x is: 4, f(x) is: 198\n",
      "\n",
      "Iteration 166: X = 2.1472010808442814, f(x) = -26.611979763452368\n",
      "Iteration 167: X = 2.147199849708826, f(x) = -26.611979764921912\n",
      "Iteration 168: X = 2.147198693530893, f(x) = -26.61197976621797\n",
      "Iteration 169: X = 2.1471976077465786, f(x) = -26.611979767361007\n",
      "Iteration 170: X = 2.1471965880698725, f(x) = -26.611979768369096\n",
      "Iteration 171: X = 2.147195630475738, f(x) = -26.61197976925817\n",
      "\n",
      "The local minimum occurs at 2.147195630475738\n"
     ]
    }
   ],
   "source": [
    "print(\"Before entering the iteration x is: \"+str(cur_x)+\", f(x) is: \"+ str(function(cur_x))+\"\\n\")\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    # Store current x value in prev_x\n",
    "    prev_x = cur_x\n",
    "    # Grad descent\n",
    "    cur_x = cur_x - rate * df(prev_x)\n",
    "    # Change in x\n",
    "    previous_step_size = abs(cur_x - prev_x) \n",
    "    # iteration count\n",
    "    iters = iters+1\n",
    "    # Iteration stopped at 171, showing last 6 Iterations\n",
    "    if iters>=166:\n",
    "        # Print iterations\n",
    "        print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x))) \n",
    "\n",
    "print(\"\\nThe local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Setting x = −4 and η = 0.01, run gradient descent for 1200 iterations. As in the previous two exercises, report the initial values of x and f(x), the next 6 values of x and f(x), and the last 6 values of x and f(x). Compare the results obtained this time to the results obtained above for x = −4 and η = 0.001. What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To find derivative: http://www.learningaboutelectronics.com/Articles/How-to-find-the-derivative-of-a-function-in-Python.php\n",
    "# The algorithm starts at x=-4\n",
    "cur_x = -4\n",
    "\n",
    "# Learning rate\n",
    "rate = 0.01 \n",
    "\n",
    "#This tells us when to stop the algorithm\n",
    "precision = 0.000001 \n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iters = 1200\n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# Gradient of our function \n",
    "df = lambda x: (8*(x**3)) - (6*(x**2)) - (24*x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before entering the iteration x is: -4, f(x) is: 454\n",
      "\n",
      "Iteration 1: X = 1.12, f(x) = -8.71561728\n",
      "Iteration 2: X = 1.35166976, f(x) = -14.187225687602176\n",
      "Iteration 3: X = 1.588129914065571, f(x) = -19.554356180837104\n",
      "Iteration 4: X = 1.8001695002820235, f(x) = -23.55150883046352\n",
      "Iteration 5: X = 1.9599549783032466, f(x) = -25.64204722189585\n",
      "Iteration 6: X = 2.0585082124451546, f(x) = -26.383081197323108\n",
      "Iteration 14: X = 2.147123260359083, f(x) = -26.611979674906664\n",
      "Iteration 15: X = 2.147158327192368, f(x) = -26.61197976044408\n",
      "Iteration 16: X = 2.147172045535117, f(x) = -26.611979773534642\n",
      "Iteration 17: X = 2.147177411923369, f(x) = -26.611979775537804\n",
      "Iteration 18: X = 2.147179511118853, f(x) = -26.611979775844326\n",
      "Iteration 19: X = 2.147180332263948, f(x) = -26.61197977589123\n",
      "\n",
      "The local minimum occurs at 2.147180332263948\n"
     ]
    }
   ],
   "source": [
    "print(\"Before entering the iteration x is: \"+str(cur_x)+\", f(x) is: \"+ str(function(cur_x))+\"\\n\")\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    # Store current x value in prev_x\n",
    "    prev_x = cur_x\n",
    "    # Grad descent\n",
    "    cur_x = cur_x - rate * df(prev_x)\n",
    "    # Change in x\n",
    "    previous_step_size = abs(cur_x - prev_x) \n",
    "    # iteration count\n",
    "    iters = iters+1\n",
    "    # Iteration stopped at 19, showing first and last 6 Iterations\n",
    "    if iters<=6:\n",
    "        # Print iterations\n",
    "        print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x)))\n",
    "    if iters>=14:\n",
    "        # Print iterations\n",
    "        print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x)))\n",
    "\n",
    "print(\"\\nThe local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Setting x = −4 and η = 0.1, run gradient descent for 100 iterations. What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To find derivative: http://www.learningaboutelectronics.com/Articles/How-to-find-the-derivative-of-a-function-in-Python.php\n",
    "# The algorithm starts at x=-4\n",
    "cur_x = -4\n",
    "\n",
    "# Learning rate\n",
    "rate = 0.1 \n",
    "\n",
    "#This tells us when to stop the algorithm\n",
    "precision = 0.000001 \n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iters = 100\n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# Gradient of our function \n",
    "df = lambda x: (8*(x**3)) - (6*(x**2)) - (24*x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before entering the iteration x is: -4, f(x) is: 454\n",
      "\n",
      "Iteration 1: X = 47.2, f(x) = 9689505.955200002\n",
      "Iteration 2: X = -82626.05440000002, f(x) = 9.321875746621314e+19\n",
      "Iteration 3: X = 451278842347294.06, f(x) = 8.294875771953852e+58\n",
      "Iteration 4: X = -7.352328532672759e+43, f(x) = 5.8442611657954e+175\n",
      "\n",
      "The local minimum occurs at 3.179542992304705e+131\n"
     ]
    }
   ],
   "source": [
    "print(\"Before entering the iteration x is: \"+str(cur_x)+\", f(x) is: \"+ str(function(cur_x))+\"\\n\")\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    try:\n",
    "        # Store current x value in prev_x\n",
    "        prev_x = cur_x\n",
    "        # Grad descent\n",
    "        cur_x = cur_x - rate * df(prev_x)\n",
    "        # Change in x\n",
    "        previous_step_size = abs(cur_x - prev_x) \n",
    "        # iteration count\n",
    "        iters = iters+1\n",
    "        # Print iterations\n",
    "        print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x)))\n",
    "    except OverflowError:\n",
    "        break\n",
    "\n",
    "print(\"\\nThe local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Shuffling too much!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
