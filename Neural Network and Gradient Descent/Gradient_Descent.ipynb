{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Model Implementation\n",
    "\n",
    "### Developed By: Akshay Tambe and Snahil Singh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/gegtl5GF3rVdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1512a982e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# creating the function and plotting it \n",
    "function = lambda x: (2*(x**4))-(2*(x**3))-(12*(x**2))+6\n",
    "\n",
    "# Get 1000 evenly spaced numbers between -4 and 4 (arbitratil chosen to ensure steep curve)\n",
    "x = np.linspace(-4,4,2000)\n",
    "\n",
    "# Plot the curve\n",
    "plt.plot(x, function(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. What is the value of x at the local minimum and at the global minimum? Find the answer to this question like either by hand with calculus or using matlab or other software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -4.348958\n",
      "         Iterations: 18\n",
      "         Function evaluations: 37\n",
      "Local Minima at x =  -1.3971679687499976\n",
      "Global Minima at x =  2.1471808637959735\n"
     ]
    }
   ],
   "source": [
    "# Verified at: https://www.symbolab.com/solver/calculus-function-extreme-points-calculator/extreme%20f%5Cleft(x%5Cright)%3D%202x%5E%7B4%7D%20%E2%88%92%202x%5E%7B3%7D%20%E2%88%92%2012x%5E%7B2%7D%20%2B%206\n",
    "# Local Minima\n",
    "from scipy.optimize import fmin\n",
    "print(\"Local Minima at x = \",fmin(function,-4)[0])\n",
    "\n",
    "# Global Minima\n",
    "from scipy import optimize\n",
    "x_min = optimize.brent(function)\n",
    "print(\"Global Minima at x = \",optimize.brent(function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Perform Gradient Descent,  Setting x = −4 and η = 0.001, run gradient descent for 6 iterations (that is, do the update 6 times). Report the values of x and f(x) at the start and after each of the first 6 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To find derivative: http://www.learningaboutelectronics.com/Articles/How-to-find-the-derivative-of-a-function-in-Python.php\n",
    "# The algorithm starts at x=-4\n",
    "cur_x = -4 \n",
    "\n",
    "# Learning rate\n",
    "rate = 0.001 \n",
    "\n",
    "#This tells us when to stop the algorithm\n",
    "precision = 0.000001 \n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iters = 6 \n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# Gradient of our function \n",
    "df = lambda x: (8*(x**3)) - (6*(x**2)) - (24*x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before entering the iteration x is: -4, f(x) is: 454\n",
      "\n",
      "Iteration 1: X = -3.488, f(x) = 240.90741220147203\n",
      "Iteration 2: X = -3.159231053824, f(x) = 148.52441854620668\n",
      "Iteration 3: X = -2.9229164225026394, f(x) = 99.4029877988204\n",
      "Iteration 4: X = -2.742031675863951, f(x) = 70.0712149441725\n",
      "Iteration 5: X = -2.59779507407776, f(x) = 51.16573699678776\n",
      "Iteration 6: X = -2.4794003442716166, f(x) = 38.29644231132754\n",
      "\n",
      "The minimum occurs at -2.4794003442716166\n"
     ]
    }
   ],
   "source": [
    "print(\"Before entering the iteration x is: \"+str(cur_x)+\", f(x) is: \"+ str(function(cur_x))+\"\\n\")\n",
    "while iters < max_iters:\n",
    "    # Store current x value in prev_x\n",
    "    prev_x = cur_x\n",
    "    # Grad descent\n",
    "    cur_x = cur_x - rate * df(prev_x)\n",
    "    # Change in x\n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "    if previous_step_size < precision:\n",
    "        print(\"X converges at Iteration \"+str(iters))\n",
    "    # iteration count\n",
    "    iters = iters+1\n",
    "    # Print iterations\n",
    "    print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x))) \n",
    "\n",
    "print(\"\\nThe minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1200 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To find derivative: http://www.learningaboutelectronics.com/Articles/How-to-find-the-derivative-of-a-function-in-Python.php\n",
    "# The algorithm starts at x=-4\n",
    "cur_x = -4 \n",
    "\n",
    "# Learning rate\n",
    "rate = 0.001 \n",
    "\n",
    "#This tells us when to stop the algorithm\n",
    "precision = 0.000001 \n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iters = 1200\n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# Gradient of our function \n",
    "df = lambda x: (8*(x**3)) - (6*(x**2)) - (24*x)\n",
    "converge = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before entering the iteration x is: -4, f(x) is: 454\n",
      "\n",
      "X converges at Iteration 250\n",
      "Iteration 1195: X = -1.3971808598447308, f(x) = -4.348957724100302\n",
      "Iteration 1196: X = -1.3971808598447308, f(x) = -4.348957724100302\n",
      "Iteration 1197: X = -1.3971808598447308, f(x) = -4.348957724100302\n",
      "Iteration 1198: X = -1.3971808598447308, f(x) = -4.348957724100302\n",
      "Iteration 1199: X = -1.3971808598447308, f(x) = -4.348957724100302\n",
      "Iteration 1200: X = -1.3971808598447308, f(x) = -4.348957724100302\n",
      "\n",
      "The minimum occurs at -1.3971808598447308\n"
     ]
    }
   ],
   "source": [
    "print(\"Before entering the iteration x is: \"+str(cur_x)+\", f(x) is: \"+ str(function(cur_x))+\"\\n\")\n",
    "while iters < max_iters:\n",
    "    # Store current x value in prev_x\n",
    "    prev_x = cur_x\n",
    "    # Grad descent\n",
    "    cur_x = cur_x - rate * df(prev_x)\n",
    "    # Change in x\n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "    if previous_step_size < precision and converge == 0:\n",
    "        print(\"X converges at Iteration \"+str(iters))\n",
    "        converge = 1\n",
    "    # iteration count\n",
    "    iters = iters+1\n",
    "    # Iteration stopped at 251, showing last 6 Iterations\n",
    "    if iters>=1195:\n",
    "        # Print iterations\n",
    "        print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x))) \n",
    "    \n",
    "print(\"\\nThe minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Repeat the previous exercise, but this time, start with x=4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To find derivative: http://www.learningaboutelectronics.com/Articles/How-to-find-the-derivative-of-a-function-in-Python.php\n",
    "# The algorithm starts at x=-4\n",
    "cur_x = 4 \n",
    "\n",
    "# Learning rate\n",
    "rate = 0.001 \n",
    "\n",
    "#This tells us when to stop the algorithm\n",
    "precision = 0.000001 \n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iters = 6\n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# Gradient of our function \n",
    "df = lambda x: (8*(x**3)) - (6*(x**2)) - (24*x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before entering the iteration x is: 4, f(x) is: 198\n",
      "\n",
      "Iteration 1: X = 3.68, f(x) = 110.61233152000005\n",
      "Iteration 2: X = 3.450886144, f(x) = 64.53629857986431\n",
      "Iteration 3: X = 3.276396901609702, f(x) = 37.31076190742675\n",
      "Iteration 4: X = 3.138067975365072, f(x) = 19.971643359608052\n",
      "Iteration 5: X = 3.0252501730040535, f(x) = 8.322601113072949\n",
      "Iteration 6: X = 2.9312689375235244, f(x) = 0.17557478693807127\n",
      "\n",
      "The minimum occurs at 2.9312689375235244\n"
     ]
    }
   ],
   "source": [
    "print(\"Before entering the iteration x is: \"+str(cur_x)+\", f(x) is: \"+ str(function(cur_x))+\"\\n\")\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    # Store current x value in prev_x\n",
    "    prev_x = cur_x\n",
    "    # Grad descent\n",
    "    cur_x = cur_x - rate * df(prev_x)\n",
    "    # Change in x\n",
    "    previous_step_size = abs(cur_x - prev_x) \n",
    "    # iteration count\n",
    "    iters = iters+1\n",
    "    # Print iterations\n",
    "    print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x))) \n",
    "\n",
    "print(\"\\nThe minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1200 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To find derivative: http://www.learningaboutelectronics.com/Articles/How-to-find-the-derivative-of-a-function-in-Python.php\n",
    "# The algorithm starts at x=-4\n",
    "cur_x = 4 \n",
    "\n",
    "# Learning rate\n",
    "rate = 0.001 \n",
    "\n",
    "#This tells us when to stop the algorithm\n",
    "precision = 0.000001 \n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iters = 1200\n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# Gradient of our function \n",
    "df = lambda x: (8*(x**3)) - (6*(x**2)) - (24*x) \n",
    "converge = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before entering the iteration x is: 4, f(x) is: 198\n",
      "\n",
      "X converges at Iteration 170\n",
      "Iteration 1195: X = 2.1471808598447315, f(x) = -26.611979775899705\n",
      "Iteration 1196: X = 2.1471808598447315, f(x) = -26.611979775899705\n",
      "Iteration 1197: X = 2.1471808598447315, f(x) = -26.611979775899705\n",
      "Iteration 1198: X = 2.1471808598447315, f(x) = -26.611979775899705\n",
      "Iteration 1199: X = 2.1471808598447315, f(x) = -26.611979775899705\n",
      "Iteration 1200: X = 2.1471808598447315, f(x) = -26.611979775899705\n",
      "\n",
      "The minimum occurs at 2.1471808598447315\n"
     ]
    }
   ],
   "source": [
    "print(\"Before entering the iteration x is: \"+str(cur_x)+\", f(x) is: \"+ str(function(cur_x))+\"\\n\")\n",
    "while iters < max_iters:\n",
    "    # Store current x value in prev_x\n",
    "    prev_x = cur_x\n",
    "    # Grad descent\n",
    "    cur_x = cur_x - rate * df(prev_x)\n",
    "    # Change in x\n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "    if previous_step_size < precision and converge == 0:\n",
    "        print(\"X converges at Iteration \"+str(iters))\n",
    "        converge = 1\n",
    "    # iteration count\n",
    "    iters = iters+1\n",
    "    # Iteration stopped at 171, showing last 6 Iterations\n",
    "    if iters>=1195:\n",
    "        # Print iterations\n",
    "        print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x))) \n",
    "\n",
    "print(\"\\nThe minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Setting x = −4 and η = 0.01, run gradient descent for 1200 iterations. As in the previous two exercises, report the initial values of x and f(x), the next 6 values of x and f(x), and the last 6 values of x and f(x). Compare the results obtained this time to the results obtained above for x = −4 and η = 0.001. What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To find derivative: http://www.learningaboutelectronics.com/Articles/How-to-find-the-derivative-of-a-function-in-Python.php\n",
    "# The algorithm starts at x=-4\n",
    "cur_x = -4\n",
    "\n",
    "# Learning rate\n",
    "rate = 0.01 \n",
    "\n",
    "#This tells us when to stop the algorithm\n",
    "precision = 0.000001 \n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iters = 1200\n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# Gradient of our function \n",
    "df = lambda x: (8*(x**3)) - (6*(x**2)) - (24*x) \n",
    "converge = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before entering the iteration x is: -4, f(x) is: 454\n",
      "\n",
      "Iteration 1: X = 1.12, f(x) = -8.71561728\n",
      "Iteration 2: X = 1.35166976, f(x) = -14.187225687602176\n",
      "Iteration 3: X = 1.588129914065571, f(x) = -19.554356180837104\n",
      "Iteration 4: X = 1.8001695002820235, f(x) = -23.55150883046352\n",
      "Iteration 5: X = 1.9599549783032466, f(x) = -25.64204722189585\n",
      "Iteration 6: X = 2.0585082124451546, f(x) = -26.383081197323108\n",
      "X converges at Iteration 18\n",
      "Iteration 1195: X = 2.147180859844728, f(x) = -26.611979775899698\n",
      "Iteration 1196: X = 2.147180859844728, f(x) = -26.611979775899698\n",
      "Iteration 1197: X = 2.147180859844728, f(x) = -26.611979775899698\n",
      "Iteration 1198: X = 2.147180859844728, f(x) = -26.611979775899698\n",
      "Iteration 1199: X = 2.147180859844728, f(x) = -26.611979775899698\n",
      "Iteration 1200: X = 2.147180859844728, f(x) = -26.611979775899698\n",
      "\n",
      "The minimum occurs at 2.147180859844728\n"
     ]
    }
   ],
   "source": [
    "print(\"Before entering the iteration x is: \"+str(cur_x)+\", f(x) is: \"+ str(function(cur_x))+\"\\n\")\n",
    "while iters < max_iters:\n",
    "    # Store current x value in prev_x\n",
    "    prev_x = cur_x\n",
    "    # Grad descent\n",
    "    cur_x = cur_x - rate * df(prev_x)\n",
    "    # Change in x\n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "    if previous_step_size < precision and converge == 0:\n",
    "        print(\"X converges at Iteration \"+str(iters))\n",
    "        converge = 1\n",
    "    # iteration count\n",
    "    iters = iters+1\n",
    "    # Iteration stopped at 19, showing first and last 6 Iterations\n",
    "    if iters<=6:\n",
    "        # Print iterations\n",
    "        print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x)))\n",
    "    if iters>=1195:\n",
    "        # Print iterations\n",
    "        print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x)))\n",
    "\n",
    "print(\"\\nThe minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Setting x = −4 and η = 0.1, run gradient descent for 100 iterations. What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To find derivative: http://www.learningaboutelectronics.com/Articles/How-to-find-the-derivative-of-a-function-in-Python.php\n",
    "# The algorithm starts at x=-4\n",
    "cur_x = -4\n",
    "\n",
    "# Learning rate\n",
    "rate = 0.1 \n",
    "\n",
    "#This tells us when to stop the algorithm\n",
    "precision = 0.000001 \n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "# maximum number of iterations\n",
    "max_iters = 100\n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# Gradient of our function \n",
    "df = lambda x: (8*(x**3)) - (6*(x**2)) - (24*x) \n",
    "converge = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before entering the iteration x is: -4, f(x) is: 454\n",
      "\n",
      "Iteration 1: X = 47.2, f(x) = 9689505.955200002\n",
      "Iteration 2: X = -82626.05440000002, f(x) = 9.321875746621314e+19\n",
      "Iteration 3: X = 451278842347294.06, f(x) = 8.294875771953852e+58\n",
      "Iteration 4: X = -7.352328532672759e+43, f(x) = 5.8442611657954e+175\n",
      "Iteration 5: X = -inf, f(x) = nan\n",
      "\n",
      "The minimum occurs at 3.179542992304705e+131\n"
     ]
    }
   ],
   "source": [
    "print(\"Before entering the iteration x is: \"+str(cur_x)+\", f(x) is: \"+ str(function(cur_x))+\"\\n\")\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    try:\n",
    "        # Store current x value in prev_x\n",
    "        prev_x = cur_x\n",
    "        # Grad descent\n",
    "        cur_x = cur_x - rate * df(prev_x)\n",
    "        # Change in x\n",
    "        previous_step_size = abs(cur_x - prev_x)\n",
    "        if previous_step_size < precision and converge == 0:\n",
    "            print(\"X converges at Iteration \"+str(iters))\n",
    "            converge = 1\n",
    "        # iteration count\n",
    "        iters = iters+1\n",
    "        # Print iterations\n",
    "        print(\"Iteration \"+str(iters)+\": X = \"+str(cur_x)+\", f(x) = \"+str(function(cur_x)))\n",
    "    except OverflowError:\n",
    "        print(\"Iteration \"+str(iters)+\": X = -inf, f(x) = nan\")\n",
    "        break\n",
    "\n",
    "print(\"\\nThe minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Shuffling too much!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
